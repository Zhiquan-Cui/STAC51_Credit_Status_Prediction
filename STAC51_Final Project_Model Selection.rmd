---
title: "Model selection"
author: "Lu Zheng"
date: "2023-03-30"
output:
  pdf_document:
    latex_engine: xelatex
    number_section: TRUE
---

# Load Required Libraries
```{r}
library(boot)
library(pROC)
library(ROCR)
library(ggplot2)
```
# Load the data
```{r}
data.credit = read.csv("Credit.csv")
# Transform categorical variables
data.credit$credit_risk = as.factor(data.credit$credit_risk)
data.credit$status  = as.factor(data.credit$status)
data.credit$savings = as.factor(data.credit$savings)
data.credit$property = as.ordered(data.credit$property)
data.credit$other_installment_plans = as.factor(data.credit$other_installment_plans)
# Remove outliers of age
data.credit = subset(data.credit, age < 60)
ggplot(data.credit, aes(x=as.factor(credit_risk), y=age, color=credit_risk)) +
      geom_boxplot() + xlab("Credit Risk")
```

# Split the data into training set and testing set
```{r}
set.seed(1006742107)

n = nrow(data.credit)
index = sample(n, round(0.75 * n), replace = FALSE)
traindata = data.credit[index, ]
testdata = data.credit[-index, ]
```

# Main effect model
## Training model
### Forward method
```{r}
step(glm(credit_risk ~ 1, family = binomial, data = traindata), scope = 
       ~status + duration + savings + property + age +
       other_installment_plans, direction = "forward", test = "Chisq")
```

### Backward method
```{r}
step(glm(credit_risk ~status + duration + savings + property + age + 
           other_installment_plans, family = binomial, data = traindata), test = "Chisq")
```


From above coding, we could find that both forward selection and backward elimination choose the model: glm(credit_risk ~status + duration + savings + property + age + other_installment_plans, family = binomial, data = traindata)

\[logit(\hat\pi)=-0.72+0.45\cdot S_1+0.86\cdot S_2+1.75\cdot S_3-0.03\cdot D+0.26\cdot SV_1+0.14\cdot SV_2+1.50SV_3+0.73SV_4-0.58\cdot P_L-0.16\cdot P_Q\]
\[-0.07\cdot P_C+0.02\cdot A+0.20\cdot O_1+0.59\cdot O_2\]

where
* $S_i$'s are dummy variables for status
* D is duration
* $SV$'s are dummy variables for savings
* $P_i$'s are dummy variables for property
* A is age
* $O_i$'s are dummy variables for other_installment_plans

```{r}
bestmodel.1 = glm(credit_risk ~status + duration + savings + property + age + other_installment_plans, family = binomial, data = traindata)
summary(bestmodel.1)
```

## Testing model
```{r}
pred.1 = predict(bestmodel.1, newdata = testdata)
plot(testdata$credit_risk, inv.logit(pred.1), xlab = "Actual credit_risk", ylab = "Predicted credit_risk")
```

From the plot, we find that the main effect model can describe the actual data fairly well.


# Interaction model
## Training model
### Forward method
```{r}
bestmodel.3 <- step(glm(credit_risk ~ 1, family = binomial, data = traindata), scope = ~status * duration * savings * property * age * other_installment_plans, direction = "forward", test = "Chisq")
```


## Testing model

```{r message=FALSE}
pred.3 <- predict(bestmodel.3, newdata = testdata)
plot(testdata$credit_risk, inv.logit(pred.3), xlab = "Actual credit_risk", ylab = "Predicted credit_risk")
roc(testdata$credit_risk~inv.logit(pred.3), plot=TRUE, main="ROC Curve", col="blue")
auc(testdata$credit_risk~inv.logit(pred.3))
```







